{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Melanie Walsh\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping — Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of lessons, we're going to introduce how to \"scrape\" data from the internet with the Python libraries requests and BeautifulSoup.\n",
    "\n",
    "We will cover how to:\n",
    "\n",
    "* Programmatically access the text of a web page\n",
    "* Understand the basics of HTML\n",
    "* Extract certain HTML elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need To Scrape At All?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/).\n",
    "\n",
    "To find their 2,000 screenplays, Hannah Andersen and Matt Daniels consulted a number of already existing sources — one of which was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red!\n",
    "\n",
    "These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:48:47.607114Z",
     "start_time": "2022-12-01T12:48:47.601020Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:11.842270Z",
     "start_time": "2022-12-01T12:44:11.820105Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:13.791279Z",
     "start_time": "2022-12-01T12:44:13.762872Z"
    },
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each movie title in this CSV file is paired with a URL for the screenplay. How can we actually use these URLs to get computationally tractable text data?\n",
    "\n",
    "Though we could manually navigate to each URL and copy/paste each screenplay into a file, that would be suuuuper slow and painstaking, and we would lose crucial data in the process — information that might help us automatically distinguish the title of the movie from the screenplay itself, for example. It would be much better to programmatically access the text data attached to every URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses and Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To programmatically access the text data attached to every URL, we can use a Python library called [requests](https://requests.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/request-response.png\" class=\"center\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:10.835489Z",
     "start_time": "2022-12-01T14:11:10.756186Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HTML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:19.486836Z",
     "start_time": "2022-12-01T12:44:18.259147Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Status Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check out `response`, it will simply tell us its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not.\n",
    "\n",
    "\"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:19.523668Z",
     "start_time": "2022-12-01T12:44:19.518177Z"
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:20.814440Z",
     "start_time": "2022-12-01T12:44:20.530847Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:20.881209Z",
     "start_time": "2022-12-01T12:44:20.875882Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Ghostboogers.png\" class=\"center\" width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:22.820816Z",
     "start_time": "2022-12-01T12:44:22.814651Z"
    }
   },
   "outputs": [],
   "source": [
    "html_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the screenplay now in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:44:48.340966Z",
     "start_time": "2022-12-01T12:44:48.334531Z"
    },
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "print(html_string[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From Multiple Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly demonstrate how we might extract the screenplay text for every URL in the DataFrame. To do so, we're going to create a smaller DataFrame from the Cornell Movie Dialogue Corpus, which consists of the first 10 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:45:53.449951Z",
     "start_time": "2022-12-01T12:45:53.445431Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_urls = urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:45:53.728896Z",
     "start_time": "2022-12-01T12:45:53.711895Z"
    },
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:45:54.462377Z",
     "start_time": "2022-12-01T12:45:54.456271Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_screenplay(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    html_string = response.text\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going apply it to the \"script_url\" column of the DataFrame and create a new column for the resulting extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:48:32.110994Z",
     "start_time": "2022-12-01T12:48:30.107028Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in sample_urls.iterrows():\n",
    "    try:\n",
    "        sample_urls.at[index, 'text'] = scrape_screenplay(row['script_url'])\n",
    "    except:\n",
    "        sample_urls.at[index, 'text'] = \"URL not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:45:58.240130Z",
     "start_time": "2022-12-01T12:45:58.223126Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame above is truncated, so we can't see the full contents of the \"text\" column. But if we print out every row in the column, we can see that we successfully extracted text for each URL (though some of these URLs returned 404 errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:46:04.073230Z",
     "start_time": "2022-12-01T12:46:03.302785Z"
    },
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "for text in sample_urls['text']:\n",
    "    print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all web pages will be as easy to scrape as these screenplay files, however. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from Genius.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Missy-Elliott.png\" class=\"center\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics.\n",
    "\n",
    "Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T12:51:45.341578Z",
     "start_time": "2022-12-01T12:51:45.132931Z"
    },
    "scrolled": true,
    "tags": [
     "output_scroll",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
    "html_string = response.text\n",
    "print(html_string[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
    "\n",
    "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parrish's website is titled \"Kittens and the TV Shows They Love.\" It can be found at the following URL: http://static.decontextualize.com/kittens.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kittens-web.png\" class=\"center\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use our requests library on this Kittens TV website, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "html_string = response.text\n",
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| HTML Tag                | Explanation                              |\n",
    "|--------------------|-------------------------------------------|\n",
    "| <\\!DOCTYPE>        | Defines document type                 |\n",
    "| <html\\>             | Defines HTML document                  |\n",
    "| <head\\>             | Main information about document    |\n",
    "| <title\\>            | Title for document          |\n",
    "| <body\\>             | Document body               |\n",
    "| <h1\\> to <h6\\>       |  Headings                    |\n",
    "| <p\\>                | Paragraph                       |\n",
    "| <br\\>               | Line break               |\n",
    "| <\\!\\-\\-comment here-\\-> | Comment                         |\n",
    "| <img\\> | Image                         |\n",
    "| <a\\> | Hyperlink                       |\n",
    "| <ul\\> | Unordered list                     |\n",
    "| <ol\\> | Ordered list                     |\n",
    "| <li\\> | List item                     |\n",
    "| <style\\> | Style information for a document                    |\n",
    "| <div\\> | Section in a document                   |\n",
    "| <span\\> | Section in a document                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Attributes, Classes, and IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML elements sometimes come with even more information inside a tag. This will often be a keyword (like `class` or `id`) followed by an equals sign `=` and a further descriptor such as `<div class=\"kitten\">`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know about tags as well as attributes, classes, and IDs because this is how we're going to extract specific HTML data with BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:01.832698Z",
     "start_time": "2022-12-01T14:11:01.680595Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) that we want to use, which will always be `\"html.parser\"` for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:25.707988Z",
     "start_time": "2022-12-01T14:11:25.025919Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
    "html_string = response.text\n",
    "\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:25.907530Z",
     "start_time": "2022-12-01T14:11:25.882526Z"
    }
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract HTML Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.find()` method to find and extract certain elements, such as a main header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:28.639552Z",
     "start_time": "2022-12-01T14:11:28.629016Z"
    }
   },
   "outputs": [],
   "source": [
    "document.find(\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want only the text contained between those tags, we can use `.text` to extract just the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:29.460345Z",
     "start_time": "2022-12-01T14:11:29.454941Z"
    }
   },
   "outputs": [],
   "source": [
    "document.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:30.187867Z",
     "start_time": "2022-12-01T14:11:30.178419Z"
    }
   },
   "outputs": [],
   "source": [
    "type(document.find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the HTML element that contains an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:31.141327Z",
     "start_time": "2022-12-01T14:11:31.132094Z"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract multiple HTML elements at a time with `.find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:34.547532Z",
     "start_time": "2022-12-01T14:11:34.538307Z"
    }
   },
   "outputs": [],
   "source": [
    "document.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:34.727376Z",
     "start_time": "2022-12-01T14:11:34.716843Z"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find_all(\"div\", attrs={\"class\": \"kitten\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-01T14:11:38.057407Z",
     "start_time": "2022-12-01T14:11:38.047880Z"
    }
   },
   "outputs": [],
   "source": [
    "document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {warning}\n",
    "Heads up! The code below will cause an error.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to extract the text from all the header2 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find_all(\"h2\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. That didn't work! In order to extract text data from multiple HTML elements, we need a `for` loop and some list-building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2_headers = document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will make an empty list called `h2_headers`.\n",
    "\n",
    "Then `for` each `header` in `all_h2_headers`, we will grab the `.text`, put it into a variable called `header_contents`, then `.append()` it to our `h2_headers` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers = []\n",
    "for header in all_h2_headers:\n",
    "    header_contents = header.text\n",
    "    h2_headers.append(header_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we transform this for loop into a *list comprehension*?\n",
    "```\n",
    "#For Loop\n",
    "h2_headers = []\n",
    "for header in all_h2_headers:\n",
    "    header_contents = header.text\n",
    "    h2_headers.append(header_contents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out the *list comprehension* answer here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "h2_headers = [header.text for header in all_h2_headers]\n",
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect HTML Elements with Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most times if you're looking to extract something from an HTML document, it's best to use your \"Inspect\" capabilities in your web browser. You can hover over elements that you're interested in and find that specific element in the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inspect.png\" class=\"center\" width=\"1000\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we hover over the main header:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inspect-h1.png\" class=\"center\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the name of **one** of the kittens and then return the text of the name (either \"Fluffy\" or \"Monsieur Whiskers\").\n",
    "\n",
    "To do so, open the web page (http://static.decontextualize.com/kittens.html) and then use your Developer Tools to find the HTML tag associated with the kitten names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we've learned a little bit about how to use BeautifulSoup to parse HTML documents. So how would we apply what we've learned to extract Missy Elliott lyrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://genius.com/Missy-elliott-work-it-lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Missy-elliott-work-it-lyrics\")\n",
    "html_str = response.text\n",
    "\n",
    "document = BeautifulSoup(html_str, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text[:1000] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the song lyrics for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What HTML element do we need to \"find\" to extract the song lyrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the song title for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What HTML element do we need to \"find\" to extract the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the producer for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the name of **one** of the kittens and then return the text of the name (either \"Fluffy\" or \"Monsieur Whiskers\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the song lyrics for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_tag = document.find_all(\"div\", attrs={\"class\":\"Lyrics__Container-sc-1ynbvzw-1 kUgSbL\"})\n",
    "\n",
    "lyrics = []\n",
    "for content in lyrics_tag:\n",
    "    line = content.get_text(separator=\" \")\n",
    "    lyrics.append(line)\n",
    "\n",
    "for content in lyrics:\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract just the song title for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_title = document.find('h1').text\n",
    "print(song_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the producer for Missy Elliott's \"Work It\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producers_div = document.find_all(\"div\", attrs={\"class\":\"HeaderCredits__List-wx7h8g-3\"}) # explain why HeaderCredits__List-wx7h8g-3\n",
    "\n",
    "for line in producers_div:\n",
    "    line_content = line.text\n",
    "    print(line_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
