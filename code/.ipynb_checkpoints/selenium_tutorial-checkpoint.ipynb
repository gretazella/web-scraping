{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7836131",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee874b-f638-4ab9-918a-510a8cd1878e",
   "metadata": {},
   "source": [
    "Traditional scraping libraries like BeautifulSoup work well for static HTML, but they fall short on dynamic sites where content loads via user interactions (e.g., scrolling, clicking filters). Selenium bridges this gap by automating a real web browser, simulating human actions like navigating pages, clicking buttons, and waiting for elements to load. In this tutorial we are going to scrape the website \"Ground News\" using Selenium and JavaScript.\n",
    "\n",
    "Ground News is a unique platform that aggregates news articles from thousands of sources, rating them for bias (left, center, right) and factuality. This makes it an ideal case study for examining media polarization, sentiment analysis, visualizing bias distribution and more.\n",
    "\n",
    "We aim to collect comprehensive information on news articles related to Artificial Intelligence, including the article titles, their summaries, and the media bias ratings associated with each source. This data will be organized and saved into a CSV file for easy analysis and future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0dae1-4f1d-4ccc-9b4f-f18595dfcd75",
   "metadata": {},
   "source": [
    "# What You'll Learn in This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6f9ff-c479-488b-83e2-579e9a76da97",
   "metadata": {},
   "source": [
    "This notebook will walk you through:\n",
    "\n",
    "- Setting up Selenium with a web driver (e.g., ChromeDriver).\n",
    "- Navigating to Ground News and interacting with its dynamic elements (e.g., searching for topics, expanding articles)\n",
    "- Extracting data such as article titles, sources, bias ratings\n",
    "- Handling common challenges like waiting for page loads, dealing with pop-ups\n",
    "\n",
    "By the end, you'll have a foundational script that you can adapt for your own DH projects, whether analyzing news trends, social media, or archival sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe411b39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import webdriver for controlling webpages\n",
    "from selenium import webdriver\n",
    "\n",
    "from chromedriver_autoinstaller import install\n",
    "install(True)\n",
    "\n",
    "# Import By class to identify and select elements on the webpage\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Options are the settings you can give to the browser\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# # Tools for waiting until certain conditions are met on the page (e.g., element becomes clickable)\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Import time to manually pause execution when needed\n",
    "import time\n",
    "\n",
    "# Import pandas to structure and store data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df301c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Settings\n",
    "opts = Options()\n",
    "\n",
    "# Optional: open the browers without showing it on your screen\n",
    "#opts.add_argument(\"--headless\")\n",
    "\n",
    "# Open the browser\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "# Open the website\n",
    "driver.get(\"https://ground.news/\")\n",
    "\n",
    "# Print the webpage's current title\n",
    "driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de1509-3ca8-4756-902c-433b231fbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common errors raised when the element you are searching for is not found or something blocks it\n",
    "# Only needed if you refer to them directly (we will do it later on in the code)\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "\n",
    "# Set default waiting time useful when calling element-searching methods (e.g., driver.find_elements)\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "# Identify the onboarding button and click on it, if present, to proceed to the home page\n",
    "try:\n",
    "    # Use CSS_SELECTOR with attribute=value\n",
    "    proceed_to_webpage_button = driver.find_element(By.CSS_SELECTOR, \"[data-testid='onboarding-close-button']\")\n",
    "\n",
    "    # Click on the button\n",
    "    proceed_to_webpage_button.click()\n",
    "\n",
    "# Proceed if you are already on the home page\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# List all the links in the navigation bar\n",
    "# This line combines CSS Selectors: find any element with class \"embla__slide\" and then find all <a> elements inside it\n",
    "links = driver.find_elements(By.CSS_SELECTOR, \".embla__slide a\")\n",
    "\n",
    "# If any link was detected using the above selectors:\n",
    "if links:\n",
    "\n",
    "    # Print the number of links found\n",
    "    print(f\"Found {len(links)} nav links:\")\n",
    "\n",
    "    # For each collected link:\n",
    "    for link in links:\n",
    "\n",
    "        # Print the link's text and the url\n",
    "        print(link.text, link.get_attribute(\"href\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac55ed-add6-491c-bb3c-f3d93d90dc6f",
   "metadata": {},
   "source": [
    "We successfully collected all the links in the website’s navigation bar using Selenium, though this was purely HTML extraction, which we could have also accomplished with BeautifulSoup. However, our next goal is to gather all articles related to Artificial Intelligence, which requires interacting with the page—specifically, clicking on the relevant link. Selenium is particularly useful here because it allows us to simulate user actions, such as clicking buttons or links, which is not possible with BeautifulSoup alone.\n",
    "\n",
    "Heads-up: In web automation with Selenium, you sometimes need JavaScript (JS) to click on buttons because not all elements are straightforward HTML elements that Selenium can interact with directly. Selenium can only click elements that are visible on the screen. If the element is off-screen, hidden inside a scrollable area, or if it is hidden by pop-ups, banners, or sticky headers, element.click() may fail. JS can scroll the element into view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6db1e-8f1e-4055-8a5c-c7d51ea58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to locate and click on \"Artificial Intelligence\" using Selenium's click() method \n",
    "try: \n",
    "\n",
    "    # Identify the topic to click on using ID\n",
    "    topic = driver.find_element(By.ID, \"header-trending-Artificial Intelligence\")\n",
    "    \n",
    "    # Click on the button\n",
    "    topic.click()\n",
    "\n",
    "# Use JavaScript to scroll the topic into view and click on it, if hidden\n",
    "except:\n",
    "\n",
    "    # Re-select element as the previous reference may no longer be valid.\n",
    "    topic = driver.find_element(By.ID, \"header-trending-Artificial Intelligence\")\n",
    "    \n",
    "    # Scroll the element into view using JavaScript\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", topic)\n",
    "\n",
    "    # CLick on the element using Javascript\n",
    "    driver.execute_script(\"arguments[0].click();\", topic)\n",
    "\n",
    "# Wait up to 10 seconds for the new url to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.url_contains(\"/interest/ai\"))\n",
    "\n",
    "# Check that you are on the correct url now\n",
    "print(driver.current_url) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e05d3-8f20-41cf-b737-873c43526f2b",
   "metadata": {},
   "source": [
    "## It worked! You are on the correct webpage.\n",
    "\n",
    "We can now examine the current webpage to identify and collect the articles displayed, either in full or partially, so that we can save them for further analysis.\n",
    "\n",
    "The page includes a “More Stories” button that loads additional articles. To capture all available stories, this button may need to be clicked (sometimes multiple times) until no new articles are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9c1dc-a444-4aae-bff0-3ff24c38d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the button \"more stories\" is available, click on it to load all articles\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        # Relocate the element after each iteration as the references become invalid once the webpage changes\n",
    "        more_button = driver.find_element(By.CSS_SELECTOR, \"[data-testid='load-more-stories-button']\")\n",
    "\n",
    "        # Click on the button\n",
    "        more_button.click()\n",
    "\n",
    "    # Force clicking with JavaScript if the button is hidden\n",
    "    except (ElementClickInterceptedException, ElementNotInteractableException): #, ElementNotInteractableException, StaleElementReferenceException\n",
    "\n",
    "        # Relocate the element\n",
    "        more_button = driver.find_element(By.CSS_SELECTOR, \"[data-testid='load-more-stories-button']\")\n",
    "\n",
    "        # Scroll into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", more_button)\n",
    "\n",
    "        # Click on it\n",
    "        driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "\n",
    "    # Stop only when there is no \"more stories\" button to click on\n",
    "    except NoSuchElementException:\n",
    "        break    \n",
    "\n",
    "    # Manually wait a few seconds for all articles to load\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a33e17-702f-4777-9742-0a6521247d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many articles are on the webpage\n",
    "article_count = len(driver.find_elements(By.CSS_SELECTOR, \"div .group\"))\n",
    "\n",
    "# Initialize a list to store articles' titles and links as dictionaries\n",
    "data = []\n",
    "\n",
    "# Loop through each article on the page by index\n",
    "for i in range(article_count):\n",
    "\n",
    "    # Locate article combining tag name and class name\n",
    "    article = driver.find_elements(By.CSS_SELECTOR, \"div .group\")[i]\n",
    "\n",
    "    # Locate url with the <a> tag\n",
    "    link = article.find_element(By.TAG_NAME, \"a\")\n",
    "\n",
    "    # Extract the url address from the element with get_attribute\n",
    "    href = link.get_attribute(\"href\")\n",
    "\n",
    "    # In this webpage, the title is usually stored in the tag <h3>\n",
    "    try:\n",
    "        title = article.find_element(By.TAG_NAME, \"h3\")\n",
    "\n",
    "    # However, sometimes it is stored in <h4> instead --> you can only know this by inspecting the webpage's html!\n",
    "    except:\n",
    "        title = article.find_element(By.TAG_NAME, \"h4\")\n",
    "\n",
    "    # Get the title's text\n",
    "    title_text = title.text\n",
    "\n",
    "    data.append({\n",
    "        \"title\": title_text,\n",
    "        \"link\": href\n",
    "    })\n",
    "\n",
    "# Convert your data to a pandas dataframe\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44256713-6bb7-46ed-8dcf-64baea6eb2e2",
   "metadata": {},
   "source": [
    "After storing the links to the individual articles, we can visit each one to extract additional information.\n",
    "\n",
    "We'll gather data related to political coverage bias and we will add a third column in the dataframe to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1022bb-6a40-430b-9565-8a6a6ddcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataframe with titles and links\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # Extract the link of each article\n",
    "    link = row['link']\n",
    "\n",
    "    # Access the webpage programmatically\n",
    "    driver.get(link)\n",
    "\n",
    "    # Locate information on bias by combining CSS SELECTORS and extract text\n",
    "    # innerText gets vsibile text, textContent gets can also get hidden text\n",
    "    bias_text = \"\".join([el.get_attribute(\"innerText\") or el.get_attribute(\"textContent\") or \"\" for el in driver.find_elements(By.CSS_SELECTOR, \"ul.list-disc li span\")])\n",
    "    \n",
    "    # If bias information is found\n",
    "    if bias_text:\n",
    "\n",
    "        # Save to a third column in the df called \"bias\"\n",
    "        df.loc[index, 'bias'] = bias_text\n",
    "\n",
    "    # If information on bias is not present in the <span> element\n",
    "    else:\n",
    "\n",
    "        # Extract \"missing information on bias\"\n",
    "        bias_text = \"\".join([el.get_attribute(\"innerText\") or el.get_attribute(\"textContent\") or \"\" for el in driver.find_elements(By.CSS_SELECTOR, \"ul.list-disc li\")])\n",
    "        \n",
    "        # Save it in the dataframe     \n",
    "        df.loc[index, 'bias'] = bias_text\n",
    "        \n",
    "# Save the updated df to csv\n",
    "df.to_csv('../data/ground_news_articles.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787fcb2-9e72-44c8-b51c-542adb565768",
   "metadata": {},
   "source": [
    "The web scraping process has completed successfully! Well done!\n",
    "\n",
    "You can find the final csv file in the `data/` folder as `ground_news_articles.csv`\n",
    "\n",
    "Open it and check that the columns contain the data as you expecte and it looks in good order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
